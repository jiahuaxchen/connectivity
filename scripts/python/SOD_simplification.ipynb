{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from osmnx import io\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "project_crs = 'epsg:3857'\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import Point, LineString, MultiPolygon, MultiPoint\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from momepy import remove_false_nodes, extend_lines\n",
    "\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "import ast  # to convert str with list to list of string\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santa barbara county, California\n",
      "data folder: c:\\Users\\danwillett\\Code\\jiahua-connectivity\\connectivity\\scripts/places/Santa_barbara_county__California\n"
     ]
    }
   ],
   "source": [
    "place = 'Santa barbara county, California'\n",
    "print(place)\n",
    "data_folder = f'{pjr_loc}/places/{place.replace(\",\", \"_\").replace(\" \", \"_\")}'\n",
    "print(f'data folder: {data_folder}')\n",
    "\n",
    "osmid_filter_file = \"../../data/bike_networks/bike_infrastructure_network.shp\"\n",
    "output_network_name = \"bike_network\"\n",
    "# osmid_filter_file = '../../data/lowstress_SB/lowstress_SB.shp'\n",
    "# output_network_name = \"low_stress\"\n",
    "# osmid_filter_file = '../../data/all_street_pfb_SB/neighborhood_ways.shp'\n",
    "# output_network_name = \"all_streets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santa barbara county, California\n",
      "data folder: c:\\Users\\danwillett\\Code\\jiahua-connectivity\\connectivity\\scripts/places/Santa_barbara_county__California\n",
      "finish to download data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = ox.graph_from_place(place, network_type='all_private')\n",
    "print('finish to download data')\n",
    "graph = ox.bearing.add_edge_bearings(graph, precision=1)\n",
    "graph_pro = ox.projection.project_graph(graph, to_crs=project_crs)\n",
    "io.save_graph_geopackage(graph_pro, filepath=f'{data_folder}/osm_data.gpkg', encoding='utf-8', directed=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4681478', '4681539', '4691899', '4864800', '11470811', '11470812', '16234677', '16237529', '16244941', '16249455', '16249584', '16249605', '16249758', '16249760', '16249791', '16249801', '16249897', '16250314', '16250472', '16250541', '16253520', '16253647', '16253754', '16254122', '16254308', '16254507', '16254658', '16254720', '16254722', '16254746', '16254757', '16254834', '16254894', '16255021', '16255482', '16255486', '16255520', '16255622', '16255629', '16255970', '16255992', '16256239', '16256292', '16256363', '16256364', '16256368', '16256783', '16256907', '16257062', '16257361', '16257451', '16257530', '16257612', '16257613', '16257633', '16257826', '16257891', '16257954', '16257987', '16258092', '16258431', '16258503', '16258751', '16258787', '16258828', '16258854', '16258926', '16258990', '16258991', '16259202', '16259246', '16259356', '16259366', '16259440', '16259477', '16259640', '16259649', '16259758', '16259900', '16260161', '16260309', '16260364', '16260381', '16260450', '16260459', '16260475', '16260480', '16260499', '16260586', '16260752', '16260994', '16261375', '16261377', '16261413', '16261573', '16261580', '16261725', '16261726', '16261791', '16262071', '16262113', '16262444', '16262621', '16262783', '16262800', '16262808', '16262871', '16262873', '16262915', '16262916', '16262926', '16262973', '16263068', '16263193', '16263301', '16263494', '16263570', '16263664', '16263665', '16263842', '16263912', '16264088', '16264222', '16264266', '16264296', '16264301', '16264303', '16264359', '16264361', '16264429', '16329361', '16329380', '16329397', '16503330', '16516049', '16972447', '16995499', '17073081', '20254286', '20254287', '20390694', '20652358', '20652359', '20652363', '20652366', '20653629', '20653631', '20653632', '20653636', '20653639', '20653640', '20653644', '20653649', '20656239', '20656244', '20656248', '20658056', '22324995', '26766702', '30615288', '30615297', '30615298', '31251276', '31449025', '31449039', '31449087', '31449163', '31449505', '31449528', '31449530', '31449541', '31449543', '31449548', '31449604', '32526861', '32741445', '32741461', '33194946', '33194947', '37392224', '37392225', '38089576', '38089666', '38093201', '38094317', '38094337', '38094338', '38094339', '38094363', '38096782', '38099403', '38131629', '38131631', '38131632', '38131633', '38131634', '38131635', '38132072', '38132074', '38132078', '38132079', '38132080', '38132082', '38132219', '38132220', '38132388', '38132389', '38132390', '38132391', '38132642', '38132643', '38132644', '38132714', '38132944', '38132945', '38132946', '38132947', '38132949', '38133151', '38133154', '38133157', '38137245', '38137246', '38137247', '38137248', '38137249', '38137250', '38137252', '38137253', '38171545', '38173131', '38173133', '38173134', '38216214', '38216216', '38216217', '38216218', '38216219', '38216220', '38229307', '38229310', '38229311', '38229314', '38229315', '38254689', '38267188', '38267926', '38267927', '38347040', '38415129', '39116729', '39116731', '39423015', '39423016', '39423063', '39498934', '39498935', '39992301', '39992315', '41062130', '41062132', '41062136', '41062138', '41497094', '41497095', '41497169', '41497195', '41497205', '41497210', '41497229', '41497494', '41497782', '41497783', '41497787', '41497788', '42044949', '42044950', '42044951', '42044952', '42045508', '42045510', '42045512', '42045655', '42045656', '42045657', '42045658', '42046395', '42047223', '42047767', '42047771', '42747566', '42919800', '43429466', '43429597', '43429603', '43429628', '43430823', '43430843', '43430868', '43430886', '43430986', '43430991', '43431260', '43437422', '43437492', '43438386', '43438559', '43438846', '43438906', '43438935', '43439388', '43440318', '43442614', '43442770', '43443118', '49394970', '49542798', '49542801', '50883825', '50977305', '53149069', '53149070', '56160117', '56160118', '69720244', '69720247', '69720250', '69720445', '69720452', '69720453', '69720456', '69720457', '69720459', '69720461', '69720462', '69720556', '69721265', '69721267', '69721274', '69721287', '69721293', '69721303', '69724028', '69724030', '69724038', '69724047', '69724051', '69724055', '69724059', '69724067', '69724077', '69724082', '69724088', '69727442', '69727444', '69727449', '69727451', '69727456', '69727458', '69727461', '69727464', '69727472', '69727478', '69727481', '69727490', '69727515', '69727524', '69727536', '69727545', '69727552', '69727556', '69727558', '69727561', '69727564', '69727566', '69727977', '69727979', '69727980', '69727981', '69728453', '69728454', '69728456', '69728457', '69728461', '69728463', '69755054', '69755061', '69756339', '69756353', '69756379', '69756436', '69760147', '69760150', '69760175', '69763693', '70331107', '70331112', '70331113', '70331118', '70331127', '70331128', '70331132', '70331148', '70331151', '70331155', '70331793', '70338486', '70338502', '70338506', '70338525', '70338528', '70338534', '70338546', '70338586', '70346777', '70346783', '70346795', '70346800', '70346807', '70348873', '70348875', '70348877', '70348887', '70348890', '70354125', '70354161', '70360352', '70360354', '74117237', '74117239', '74117247', '74117442', '74117443', '77096283', '77096288', '78594766', '78594768', '81379830', '103259056', '103259057', '103259058', '103259059', '103259060', '103259061', '103259062', '103259063', '103259064', '103270016', '105857710', '105857727', '106444958', '106444964', '106455821', '106466137', '106468523', '106469949', '106471830', '106475574', '106476011', '106606944', '106609137', '106609138', '106609141', '106704911', '106733737', '106733738', '106733739', '106733740', '114619522', '117601467', '119401961', '119524635', '119524722', '120096310', '120491018', '120491019', '120491020', '120622613', '120622948', '120623018', '121294975', '123032895', '125839155', '125839156', '125839157', '125839158', '125839185', '125839186', '125839187', '133349509', '133534911', '133534913', '133534914', '133534915', '136753581', '140798407', '140798408', '144055903', '144055904', '144056513', '144056516', '144056517', '144056518', '144056520', '144056849', '144056850', '144056852', '161009703', '165153855', '165153857', '165153874', '166971918', '167777254', '167777255', '172047193', '172071312', '172075757', '172259767', '185029793', '185232085', '185244596', '185244602', '185244607', '185244609', '185244614', '185244639', '186248322', '186248323', '186248343', '186248352', '186248353', '186248354', '186248358', '186248359', '186248366', '186248679', '186430811', '186430829', '186430830', '186430834', '186430836', '192668515', '192668516', '197886258', '202584593', '202637162', '202637171', '205258667', '206995079', '206995083', '207000952', '207000953', '207705852', '207949525', '207952370', '210670419', '210933415', '210933419', '210933429', '220078732', '220078736', '220078738', '220078745', '220078746', '220539862', '220770390', '220771881', '220773840', '220836644', '231771838', '231846783', '233583321', '234852680', '243734949', '243734950', '243734958', '243734959', '243735308', '244634706', '251403479', '251411820', '280128527', '280128528', '280128530', '280128534', '280128535', '280128536', '280128681', '280128682', '280128683', '292797688', '305209813', '306461989', '306461993', '306461994', '306468277', '322886708', '322886709', '329808844', '340559234', '349212489', '349829204', '349829209', '349833646', '349869817', '354392118', '354673979', '354673980', '355822462', '356425886', '356425887', '360558373', '373692660', '373692661', '373692662', '373692665', '373692666', '373692667', '373803855', '373803856', '373803857', '373803858', '373803860', '373803861', '373803862', '421308351', '421308353', '433149437', '434926184', '434926191', '437355094', '437355095', '437355096', '437355097', '437461028', '437602404', '446745485', '446745486', '446745488', '446745491', '446745496', '446745497', '446745499', '446745500', '455471121', '462437845', '462437848', '464265222', '464265224', '471171740', '471171744', '471171749', '471171753', '471171756', '471171761', '471171766', '471358182', '471358184', '471358189', '471358192', '471358195', '471844788', '471844791', '472439666', '472439667', '472439668', '472439669', '472439670', '472439672', '481844082', '483556906', '490590642', '493449978', '493449979', '493449981', '493449982', '497673981', '497673982', '497850086', '497850087', '497850091', '497850093', '497850094', '497850095', '529805381', '532629998', '542529830', '544435607', '546048728', '546048730', '549612643', '550948125', '551630572', '551630574', '551630575', '551630577', '551650734', '551650742', '551662119', '551668133', '552027539', '552067218', '552084757', '552084759', '552084761', '554111587', '556250821', '556601312', '556601315', '558181814', '560648820', '561820405', '561820410', '561820417', '561820419', '561820420', '561820421', '561820423', '561820425', '561820430', '561820432', '561820433', '561820442', '561820448', '561820451', '561820452', '561820453', '561820454', '561820455', '561820456', '561820457', '561820458', '561820459', '561820466', '561820472', '561820476', '561820477', '561820478', '561820479', '561820480', '561820482', '561820486', '561820489', '561820494', '561820498', '561820502', '561820506', '561820514', '561820518', '561820522', '561820524', '561820527', '561820528', '561820529', '561820530', '561820531', '561820532', '561860944', '561860947', '561860948', '563671921', '563671923', '563671924', '563671925', '566838556', '574424565', '577670985', '579067829', '579067830', '586190697', '586591505', '588882729', '591279065', '591503730', '591503738', '591503749', '591503756', '591503769', '591503774', '591506188', '593616199', '593616205', '593616212', '593616213', '593616215', '593629974', '593642863', '593642872', '593642877', '593642882', '593642891', '593642900', '601930249', '601930253', '601930257', '601930276', '601930282', '601931387', '601931393', '601932224', '601932228', '601932229', '601932233', '601932236', '601949545', '601949549', '601952532', '601952557', '601952565', '610628339', '610636582', '610636583', '610636584', '610912379', '612095936', '612108626', '613489667', '613491069', '613491071', '613961751', '614556470', '614556471', '614575107', '617242056', '617244341', '617247758', '617247759', '617249040', '617252604', '618369636', '618370125', '618685131', '618685132', '619574248', '619578545', '621836338', '621836339', '621836340', '621836341', '621836342', '621836344', '622145224', '622146859', '622146860', '622147562', '622475876', '623956431', '623956432', '623958320', '624128879', '624128880', '624128881', '624137868', '624137869', '624145454', '624253525', '624257131', '624262738', '624800906', '624804881', '624812679', '624819559', '624819567', '624832199', '624844529', '627887413', '627887414', '627887416', '627887419', '630791827', '634580698', '638015006', '638015010', '646157619', '646157620', '646157622', '646157623', '646157625', '646157626', '646157628', '646157629', '647107311', '647107312', '647107313', '647107314', '653656060', '653656061', '653656062', '653656063', '653656064', '653656068', '654311561', '654311562', '658672412', '658957011', '658957013', '658957014', '659200720', '659216947', '659216948', '659216949', '659216950', '666368593', '666368594', '666368595', '666368596', '667852836', '668784090', '668812127', '668812128', '668983154', '668983156', '668986999', '668991938', '668991939', '668991940', '669046410', '669250500', '669461534', '669461535', '672469867', '672469871', '672565796', '672565799', '672565800', '672565801', '679153844', '679153851', '679153860', '679153869', '681739698', '682941925', '685018044', '685018045', '685348085', '685871366', '685871367', '689799750', '690345589', '690345640', '690345641', '690883755', '694141214', '694511562', '695008531', '698765561', '698767862', '698793999', '698794287', '698794415', '701190311', '701194691', '701195891', '701199248', '701200981', '701201217', '701202431', '701205886', '701206195', '701206379', '701206880', '701209557', '701210224', '702570418', '702570419', '702570422', '702570423', '703197143', '708954496', '708954497', '708954498', '708954500', '709075317', '709075318', '709075319', '709075320', '711547355', '711547356', '711549021', '711549586', '711549587', '711550595', '711553467', '711553468', '720964538', '728219915', '728852360', '728860393', '731939483', '737982845', '737982846', '737982849', '737982850', '737982851', '737982852', '737989069', '737989070', '737989071', '737989072', '737989073', '737989074', '737989075', '737989076', '738015896', '738015900', '738015901', '738015904', '738015905', '738015908', '738015910', '738015911', '738015912', '738015913', '738015914', '738015915', '738015916', '738015917', '738015919', '738015920', '738015921', '738015922', '738015923', '738015924', '738015925', '738486101', '738486103', '740625845', '745013118', '745129086', '745132395', '753841239', '753841240', '753846387', '754180972', '754180973', '759714528', '759714529', '759780379', '759780384', '759780386', '759780387', '759780389', '759780390', '759780391', '759780392', '759780401', '759780402', '760399079', '760399080', '760413998', '760413999', '761125040', '761225934', '761225936', '761225937', '761225938', '761225939', '761225940', '761225941', '761225944', '761225954', '761225955', '776378235', '776378236', '778970261', '779223072', '779223077', '779223080', '779223081', '779223097', '779234100', '779234111', '779259007', '779463134', '779820057', '781658796', '783749495', '783749497', '784128848', '784128849', '784145763', '784379989', '784379990', '784998080', '784998087', '785503644', '789728074', '789728076', '789728077', '789728078', '789728079', '789728080', '789728081', '789728082', '789728083', '789728085', '800174462', '801773469', '801773470', '802037826', '806159449', '806159450', '806657073', '809619546', '809871172', '809871259', '809871424', '809871425', '809871487', '813523733', '813523734', '813523735', '813523800', '813523833', '813523834', '813523835', '816621813', '817278323', '817278325', '817278326', '817278336', '817278337', '817663034', '817739170', '817739171', '818225894', '823576921', '824717447', '825889685', '825889686', '825971102', '826843963', '826843964', '826843965', '826843966', '826843967', '827077209', '827077210', '827149841', '827149842', '827149843', '827149844', '827149846', '827149847', '827149848', '827418546', '827418547', '827418548', '828014486', '828311244', '828311245', '828311249', '828311250', '828311251', '828557047', '828557048', '828562491', '828596767', '828596768', '828869999', '828870000', '828879731', '828879733', '828959580', '829005124', '829179498', '829179499', '829179533', '829187960', '829187961', '829201978', '829392852', '829495874', '829495880', '829499812', '829499813', '829499814', '829499815', '829499816', '829499817', '829499818', '829499819', '829514058', '829514062', '829514424', '829514425', '829879595', '830688077', '830688079', '830688080', '830688081', '830688094', '830688095', '830688096', '830688097', '830688098', '830688099', '830688100', '830688101', '830688102', '830688103', '830688104', '830688105', '830688106', '830688107', '830688108', '830688110', '830688111', '830688112', '830688113', '830688118', '830688119', '830688120', '830688121', '830688122', '830688123', '830688124', '830688126', '834375622', '835417657', '835417658', '835637167', '835646860', '835649258', '839085333', '839085334', '839090835', '839993912', '842297468', '842475359', '842475360', '842482668', '842482674', '842847613', '842847614', '842895117', '842902729', '842902730', '844680839', '844874259', '844874261', '845183229', '845183232', '845208975', '851667350', '851948827', '853855476', '853855477', '853855478', '853855480', '854162371', '854162372', '854823421', '854823422', '854870805', '855236696', '864756849', '865979024', '867624846', '867624847', '868500315', '868500316', '868500317', '868500516', '868896836', '872230522', '872230523', '872827018', '872827019', '872827020', '872827021', '872827022', '878780486', '880439602', '881707553', '881707555', '881707558', '881707560', '881707561', '881721881', '881721882', '881721883', '881721885', '881721886', '881721889', '881726119', '887404417', '887404422', '887404423', '887404424', '887404425', '887404426', '887404427', '887725946', '887725947', '887725948', '890922620', '894393657', '894393661', '896823059', '896823060', '896882522', '896882525', '896882526', '896882529', '896882530', '896882531', '896882532', '897217801', '897217802', '897217803', '897217804', '899952715', '899952716', '899952717', '899952718', '899990784', '899990785', '901024567', '901024570', '901024571', '901024572', '901024573', '901267984', '901267985', '901267986', '901267987', '901267988', '901267989', '901276715', '902292690', '902292691', '902292692', '902292693', '908236365', '908237606', '908334099', '916206780', '916206782', '916206783', '916206784', '916563820', '916563821', '917791575', '917791576', '918219858', '918813806', '923698570', '923698571', '923698577', '923698578', '923698579', '923698580', '926152955', '928167247', '931596434', '931596435', '932271460', '932280377', '932280378', '932280379', '932289615', '933122602', '933149099', '933149100', '933149101', '933149103', '933732878', '933732879', '933732880', '933807640', '933807644', '933807645', '933817314', '933817315', '933817316', '933817317', '933822802', '933827857', '933832353', '933832354', '933832355', '933837333', '933847469', '933855993', '933855994', '933855997', '933860108', '933860109', '933861768', '933861769', '933861770', '933894162', '933894164', '933894166', '933894168', '933908311', '933908314', '933908315', '933922249', '933922251', '933922252', '933927819', '933927821', '933927823', '933996277', '933996278', '934000758', '934132145', '934136917', '934136918', '934138086', '936532157', '944527362', '946668298', '946697445', '946700605', '946700606', '946700607', '946703947', '948322787', '948322788', '948322814', '948322816', '948322830', '948322832', '948586692', '948586695', '948586696', '948589499', '948589500', '948589501', '948621894', '951580769', '957702627', '957702628', '957702640', '957702643', '963345825', '963345826', '963345827', '963345828', '963345829', '963349668', '963506689', '963506690', '963589244', '963589246', '963592200', '963592201', '963786376', '963786378', '963880967', '971722546', '977049677', '977058485', '977058486', '977058487', '977064390', '977064391', '977064392', '977064393', '977064394', '977064395', '977064396', '977064397', '977064398', '977064399', '977064400', '977064401', '977064402', '977064403', '977064404', '977064405', '977066929', '977069073', '977069074', '986884776', '988559437', '988559438', '988559439', '988559455', '988559457', '988559458', '988559459', '988559463', '988559474', '988559475', '989691542', '989691543', '989691544', '989691545', '1010180230', '1010180231', '1010180232', '1015955760', '1015955761', '1015955762', '1015955763', '1019985999', '1019986004', '1020206064', '1020206065', '1025024703', '1025024704', '1025024705', '1033107462', '1033107463', '1033113293', '1033113300', '1047382282', '1047382283', '1047382284', '1047382285', '1047383565', '1047817518', '1047824600', '1047831437', '1049215110', '1049215112', '1049221824', '1061523506', '1061523507', '1061523509', '1061523511', '1061523513', '1061523515', '1061523520', '1064246764', '1067461724', '1069408366', '1081278343', '1081278344', '1096771203', '1096771204']\n",
      "88508 before osmids\n",
      "4182 after osmids... length of network_gdf is  1535\n",
      "calculate simplification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/214 [00:00<00:05, 36.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not simplifying roads with no names\n",
      "added no roads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:55<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_parallel: 1594\n",
      "create new files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg', layer='edges')\n",
    "is_junction = True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf = my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]\n",
    "\n",
    "highway_types_remove = ['motoway', 'trunk', 'motorway_link', 'trunk_link']\n",
    "def check_highway_types(row):\n",
    "    if isinstance(row, list):\n",
    "        return any(value in row for value in highway_types_remove)\n",
    "    else:\n",
    "        return row in highway_types_remove\n",
    "\n",
    "# Filter rows based on the condition\n",
    "to_remove = my_gdf[~my_gdf['highway'].apply(check_highway_types)]\n",
    "to_remove = to_remove[~((to_remove['name'] == \"Chumash Highway\") | (to_remove['name'] == \"El Camino Real\"))]\n",
    "\n",
    "# load osmids from network file and remove streets with non-matching osmids\n",
    "network_gdf = gpd.read_file(osmid_filter_file)\n",
    "if output_network_name == \"bike_network\":\n",
    "    # bike_networks: only use roads with canbics classifications of 1-3\n",
    "    desired_comfort_levels = ['1. High Comfort', '2. Medium Comfort', '3. Low Comfort']\n",
    "    network_gdf = network_gdf[network_gdf['Can_BICS_c'].isin(desired_comfort_levels)]\n",
    "    osmids = list(pd.unique(network_gdf['osm_id']))\n",
    "elif output_network_name == \"low_stress\" or output_network_name == \"all_streets\":\n",
    "    network_gdf = network_gdf[network_gdf['XWALK'] != 1]\n",
    "    osmids = list(pd.unique(network_gdf['OSM_ID'].astype(int).astype(str)))\n",
    "\n",
    "\n",
    "print(osmids)\n",
    "print(len(to_remove), \"before osmids\")\n",
    "to_remove = to_remove[to_remove['osmid'].str.contains('|'.join(osmids))]\n",
    "print(len(to_remove), \"after osmids... length of network_gdf is \", len(network_gdf))\n",
    "# Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.\n",
    "df_pro = to_remove.to_crs(project_crs).dropna(subset=['name'])\n",
    "df_pro = df_pro[df_pro['name'] != '']\n",
    "df_pro = to_remove.to_crs(project_crs)\n",
    "df_pro['angle'] = df_pro['bearing'].apply(lambda x: x if x < 180 else x - 180)\n",
    "df_pro['length'] = df_pro.length\n",
    "\n",
    "\n",
    "# Function to convert valid list strings to lists\n",
    "def convert_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)[0]\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "        return s  # Return the original string if conversion fails\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame column so polylines with several street names will return the first name and highway type.\n",
    "df_pro['name'] = df_pro['name'].apply(convert_to_list)\n",
    "df_pro['highway'] = df_pro['highway'].apply(convert_to_list)\n",
    "\n",
    "\n",
    "# Functions and classes to be utilized - Module 2\n",
    "def check_parallelism(to_translate: GeoDataFrame) -> bool:\n",
    "    # See if there are parallel lines\n",
    "    my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "    to_translate['geometry_right'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "    to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35,\n",
    "                                                                                                'left'))  # we need to offset by both sides since the parallel lines could be in opposite directions\n",
    "\n",
    "    def is_parallel(my_s_join: GeoDataFrame, the_buffer: GeoSeries, geo_field: str):\n",
    "        my_s_join['geometry'] = my_s_join[geo_field]\n",
    "        new_data_0 = my_s_join.sjoin(GeoDataFrame(geometry=the_buffer, crs=project_crs), how='inner').reset_index()\n",
    "        if not len(new_data_0):\n",
    "            return False\n",
    "        new_data_1 = new_data_0[\n",
    "            new_data_0['index'] != new_data_0['index_right']]  # Remove overlay of polylines with its buffer\n",
    "        for translated_line in new_data_1.iterrows():\n",
    "            translated_line = translated_line[1]\n",
    "            geo_tr_line = GeoDataFrame(data=pd.DataFrame([translated_line]), crs=project_crs)\n",
    "            overlay = gpd.overlay(geo_tr_line, GeoDataFrame(geometry=the_buffer.loc[geo_tr_line['index_right']],\n",
    "                                                            crs=project_crs), how='intersection')\n",
    "            if (overlay.length / translated_line.length).iloc[0] * 100 > 10:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if is_parallel(to_translate, my_buffer, 'geometry_right'):\n",
    "        return True\n",
    "    else:\n",
    "        if is_parallel(to_translate, my_buffer, 'geometry_left'):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def create_center_line(one_poly):\n",
    "    \"\"\"\n",
    "    This method calculate new line between the farthest points of the simplified polygon\n",
    "    :param one_poly:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lines_in_buffer = data.sjoin(GeoDataFrame(geometry=[one_poly], crs=project_crs)).drop(columns='index_right')\n",
    "\n",
    "    list_pnts_of_line_group = []\n",
    "\n",
    "    def update_list(line_local):\n",
    "        \"\"\"\n",
    "        add the first start/end point into the list\n",
    "        :param line_local:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        list_pnts_of_line_group.extend([Point(line_local.coords[0]), Point(line_local.coords[-1])])\n",
    "\n",
    "    # Get the start/end points of these polylines\n",
    "    lines_in_buffer['geometry'].apply(update_list)\n",
    "\n",
    "    # Find all the unidirectional combinations between each two pair of points\n",
    "    point_combinations = list(combinations(list_pnts_of_line_group, 2))\n",
    "\n",
    "    # Save it into DataFrame frame and calculate distance\n",
    "    df_test = DataFrame()\n",
    "    df_test['point_1'] = [pair[0] for pair in point_combinations]\n",
    "    df_test['point_2'] = [pair[1] for pair in point_combinations]\n",
    "    df_test['dist'] = df_test.apply(lambda x: x['point_1'].distance(x['point_2']), axis=1)\n",
    "\n",
    "    # Calculate  angle (0 and 180)\n",
    "    # Calculate angle using vectorized operations\n",
    "    # Vectorized angle calculation using NumPy\n",
    "    dx = df_test['point_2'].apply(lambda p: p.x) - df_test['point_1'].apply(lambda p: p.x)\n",
    "    dy = df_test['point_2'].apply(lambda p: p.y) - df_test['point_1'].apply(lambda p: p.y)\n",
    "    df_test['angle'] = np.degrees(np.arctan2(dy, dx))\n",
    "    df_test['angle'] = np.where(df_test['angle'] > 0, df_test['angle'], df_test['angle'] + 180)\n",
    "\n",
    "    # Calculate the best two points by looking on their distance and angle. we compare the angle to the polylines angles. The angle has less important so the reason for 0.5\n",
    "    avg = lines_in_buffer['angle'].mean()\n",
    "    dis = abs(df_test['angle'] - avg)\n",
    "    df_test['ratio'] = df_test['dist'] / df_test['dist'].max() + 0.5 * dis / dis.max()\n",
    "    max_points = df_test.sort_values(by='ratio', ascending=False).iloc[0]\n",
    "\n",
    "    # These points will be served to be initial reference in order to find more points\n",
    "    pnt_f = max_points['point_1']\n",
    "    pnt_l = max_points['point_2']\n",
    "\n",
    "    angl_rng = lines_in_buffer['angle'].max() - lines_in_buffer['angle'].min()\n",
    "    if angl_rng < 1:  # If the angel range is less than 1 degree the line will be based on the first and last points\n",
    "        lines_pnt_geo = [pnt_f]\n",
    "    else:\n",
    "        if angl_rng > 100:  # Maximum of length to check is every 10 meters\n",
    "            length_to_check = 10\n",
    "        else:\n",
    "            length_to_check = 75 - log2(\n",
    "                angl_rng) * 10  # The range of  length_to_check (logarithm to create more changes at the beginning)\n",
    "        lines_pnt_geo = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f], length_to_check, lines_in_buffer)\n",
    "    lines_pnt_geo.append(pnt_l)\n",
    "    # Update dic_final\n",
    "    return lines_pnt_geo\n",
    "\n",
    "\n",
    "def add_more_pnts_to_new_lines(pnt_f_loc: Point, pnt_l_loc: Point, line_pnts: list, lngth_chck: float,\n",
    "                                test_poly: GeoDataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This method checks if more points should be added to the new lines by checking along the new line if the distance to the old network roads are more than 10 meters\n",
    "    :param test_poly: From these polylines find the closet one in each interation\n",
    "    :param lngth_chck: Used latter to find how many checks should be done\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Calculate distance and azimuth between the first and last point\n",
    "    dist = pnt_f_loc.distance(pnt_l_loc)\n",
    "    x_0 = pnt_f_loc.coords[0][0]\n",
    "    y_0 = pnt_f_loc.coords[0][1]\n",
    "    bearing = math.atan2(pnt_l_loc.coords[0][0] - x_0, pnt_l_loc.coords[0][1] - y_0)\n",
    "    bearing = bearing + 2 * math.pi if bearing < 0 else bearing\n",
    "    # Calculate the number of  checks going to carry out\n",
    "    loops = int(dist / lngth_chck)\n",
    "    # Calculate  the first point over the line\n",
    "    for dis_on_line in range(1, loops):\n",
    "        x_new = x_0 + lngth_chck * dis_on_line * math.sin(bearing)\n",
    "        y_new = y_0 + lngth_chck * dis_on_line * math.cos(bearing)\n",
    "        # S_joins to all the network lines (same name and group)\n",
    "        # if the distance is less than 10 meters continue, else: find the projection point and add it to the correct location and run the function agein\n",
    "        one_pnt_df = GeoDataFrame(geometry=[Point(x_new, y_new)], crs=project_crs)\n",
    "        s_join_loc = one_pnt_df.sjoin_nearest(test_poly, distance_col='dis').iloc[0]\n",
    "\n",
    "        if s_join_loc['dis'] > 10:\n",
    "            line = data.loc[s_join_loc['index_right']]['geometry']\n",
    "            pnt_med = line.interpolate(line.project(s_join_loc['geometry']))\n",
    "            if pnt_med.distance(pnt_f_loc) < 10:  # Otherwise the code may stack in endless loops\n",
    "                continue\n",
    "            line_pnts.append(pnt_med)\n",
    "            line_pnts = add_more_pnts_to_new_lines(pnt_med, pnt_l_loc, line_pnts, lngth_chck, test_poly)\n",
    "            return line_pnts\n",
    "    return line_pnts\n",
    "\n",
    "\n",
    "def update_df_with_center_line(new_line, is_simplified=0, group_name=-1):\n",
    "    \"\"\"\n",
    "    update our dictionary with new lines\n",
    "    :param is_simplified:\n",
    "    :param new_line:\n",
    "    :param group_name: According to the DBSCAN algorithm, if no =-1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    # dic_final['geometry'].append(LineString(coordinates=(pnt_list[max_dis[0]], pnt_list[max_dis[1]])))\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)\n",
    "\n",
    "\n",
    "# Initiate dic_final here for @def update_df_with_center_line\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [], 'is_simplified': []}\n",
    "\n",
    "# group the street segments by street name\n",
    "my_groupby = df_pro.groupby('name')\n",
    "\n",
    "for_time = len(my_groupby)\n",
    "number_of_parallel = 0  # count the number of polylines were refined\n",
    "print('calculate simplification')\n",
    "with tqdm(total=for_time) as pbar:  # It is used in order to visualise the progress by progress bar\n",
    "    for i, street in enumerate(my_groupby):\n",
    "\n",
    "        res = street[1]  # it holds all the streets\n",
    "        name = street[0]  # It holds the streets name)\n",
    "        pbar.update(1)  # for the progress bar\n",
    "        if name == '' or name is None:\n",
    "            print(\"not simplifying roads with no names\")\n",
    "            data = res\n",
    "            _ = res['geometry'].apply(lambda x: update_df_with_center_line(x))\n",
    "            print(\"added no roads\")\n",
    "            continue \n",
    "\n",
    "        # Remove segments without angle. If less than two segments being left move to the next group.\n",
    "        res = res.dropna(subset=['angle'], axis=0)\n",
    "        if len(res) < 2:\n",
    "            data = res\n",
    "            _ = res['geometry'].apply(lambda x: update_df_with_center_line(x))\n",
    "            continue\n",
    "        # Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed\n",
    "        res['group'] = DBSCAN(eps=10, min_samples=2).fit(res['angle'].to_numpy().reshape(-1, 1)).labels_\n",
    "        # if all is -1, don't touch the element\n",
    "        if (res['group'] == -1).all():\n",
    "            data = res\n",
    "            _ = res['geometry'].apply(lambda x: update_df_with_center_line(x))\n",
    "            continue\n",
    "        # cur_group = res[(res['group'] > -1) | (res.length>20)].groupby('group') # Remove short segments with -1 classification values\n",
    "        # The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "        lcid = 0\n",
    "        for group in res.groupby('group'):\n",
    "            \n",
    "            data = group[1]\n",
    "            if group[0] == -1:  # No need to check if is parallel\n",
    "                _ = data['geometry'].apply(lambda x: update_df_with_center_line(x))\n",
    "                continue\n",
    "            if check_parallelism(data.copy()):\n",
    "                # Remove unimportant streets which appear more than 10% in the group\n",
    "                min_num_of_polylines = len(data) / 10\n",
    "                # Use a single boolean condition for filtering\n",
    "                condition = (data['highway'].isin(['service', 'unclassified'])) & (\n",
    "                        data.groupby('highway')['highway'].transform('count') <= min_num_of_polylines)\n",
    "                data = data[~condition]\n",
    "\n",
    "                number_of_parallel += len(data)  # Update the number of parallel polylines\n",
    "\n",
    "                # unify lines to one polygon\n",
    "                buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "\n",
    "                one_buffer = buffers.unary_union\n",
    "                \n",
    "                # simplify polygon with simplify function. If one_buffer is multipolygon object simplify each one them separately\n",
    "                if isinstance(one_buffer, MultiPolygon):\n",
    "                    \n",
    "                    for polygon in one_buffer.geoms:\n",
    "                        \n",
    "                        lines_pnt_geo_final = create_center_line(polygon)\n",
    "                        update_df_with_center_line(LineString(lines_pnt_geo_final), 1, group[0])\n",
    "                else:\n",
    "                        \n",
    "                    lines_pnt_geo_final = create_center_line(one_buffer)\n",
    "                    # Update dic_final\n",
    "                    update_df_with_center_line(LineString(lines_pnt_geo_final), 1, group[0])\n",
    "\n",
    "            else:\n",
    "                _ = data['geometry'].apply(lambda x: update_df_with_center_line(x))\n",
    "\n",
    "                \n",
    "print(f'number_of_parallel: {number_of_parallel}')\n",
    "print('create new files')\n",
    "# remove short lines\n",
    "final_cols = ['name', 'geometry', 'highway', 'bearing', 'length']\n",
    "new_network = GeoDataFrame(dic_final, crs=project_crs)\n",
    "new_network['length'] = new_network.length\n",
    "# create network\n",
    "new_network.to_file(f'{data_folder}/{output_network_name}_simp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to be employed during the execution of this code.\n",
    "# Intersection\n",
    "# Split in intersection\n",
    "class Intersection:\n",
    "    def __init__(self, network: GeoDataFrame, number: int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param network:\n",
    "        :param number: give a unique name to the files created during the process (this class will be use again in this code)\n",
    "        \"\"\"\n",
    "        self.my_network = network\n",
    "        self.inter_pnt_dic = {'geometry': [], 'name': []}\n",
    "        self.lines_to_delete = []\n",
    "        self.num = number\n",
    "        print('Update topology')\n",
    "\n",
    "    def intersection_network(self):\n",
    "        \"\"\"\n",
    "        This function fix topology (add or remove vertices) where needed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # First remove_false_nodes\n",
    "        self.my_network = remove_false_nodes(self.my_network).reset_index(drop=True)\n",
    "        # Create buffer around each element\n",
    "        buffer_around_lines = self.my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "        # s_join between buffer to lines\n",
    "        s_join_0 = gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines, crs=project_crs),\n",
    "                                right_df=self.my_network)\n",
    "\n",
    "        # delete lines belong to the buffer\n",
    "        s_join = s_join_0[s_join_0.index != s_join_0['index_right']]\n",
    "\n",
    "        # Find new intersections that are not at the beginning or end of the line\n",
    "        for_time = len(s_join)\n",
    "        with tqdm(total=for_time) as pbar:\n",
    "            s_join.apply(lambda x: self.find_intersection_points(x, pbar), axis=1)\n",
    "        if len(self.inter_pnt_dic) == 0:\n",
    "            return\n",
    "        inter_pnt_gdf = GeoDataFrame(self.inter_pnt_dic, crs=project_crs)\n",
    "        # Split string line by points\n",
    "        segments = {'geometry': [], 'org_id': []}\n",
    "        # Groupby points name (which is the line they should split)\n",
    "        my_groups = inter_pnt_gdf.groupby('name')\n",
    "        for_time = len(my_groups)\n",
    "        with  tqdm(total=for_time) as pbar:\n",
    "            for group_pnts in my_groups:\n",
    "                pbar.update(1)\n",
    "                points = group_pnts[1]\n",
    "                points['is_split'] = True\n",
    "                # if group_pnts[0]==588:\n",
    "                #     print(points)\n",
    "                # get the line to split by comparing the name\n",
    "                row = self.my_network.loc[group_pnts[0]]\n",
    "                current = list(row.geometry.coords)\n",
    "                points_line = [Point(x) for x in current]\n",
    "                points_line_gdf = GeoDataFrame(geometry=points_line, crs=project_crs)\n",
    "                points_line_gdf['is_split'] = False\n",
    "\n",
    "                # append all the points together (line points and split points)\n",
    "                line_all_pnts = GeoDataFrame(pd.concat([points_line_gdf, points]), crs=project_crs)\n",
    "\n",
    "                # Find the distance of each point form the begining of the line on the line.\n",
    "                line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(\n",
    "                    lambda x: row.geometry.project(x))\n",
    "                line_all_pnts.sort_values('dis_from_the_start', inplace=True)\n",
    "\n",
    "                # split the line\n",
    "                seg = []\n",
    "                for point in line_all_pnts.iterrows():\n",
    "                    prop = point[1]\n",
    "                    seg.append(prop['geometry'])\n",
    "                    if prop['is_split']:\n",
    "                        segments['geometry'].append(LineString(seg))\n",
    "                        segments['org_id'].append(row.name)\n",
    "                        seg = [prop['geometry']]\n",
    "                # if the split point is the last one, you don't need to create new segment\n",
    "                if len(seg) > 1:\n",
    "                    segments['geometry'].append(LineString(seg))\n",
    "                    segments['org_id'].append(row.name)\n",
    "        network_split = GeoDataFrame(data=segments, crs=project_crs)\n",
    "        cols_no_geometry = self.my_network.columns[:-1]\n",
    "        network_split_final = network_split.set_index('org_id')\n",
    "        network_split_final[cols_no_geometry] = self.my_network[cols_no_geometry]\n",
    "        # remove old and redundant line from our network and update with new one\n",
    "        network_split = GeoDataFrame(pd.concat([self.my_network.drop(index=network_split_final.index.unique()),\n",
    "                                                network_split_final]), crs=project_crs)\n",
    "        network_split['length'] = network_split.length\n",
    "        self.my_network = network_split\n",
    "        self.my_network.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def find_intersection_points(self, row, pbar):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            line_1 = self.my_network.loc[row.name]\n",
    "            line_2 = self.my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            # If there are more than one intersection between two lines, one of the lines should be deleted.\n",
    "            if isinstance(pnt,\n",
    "                            LineString):  # The intersection is only between the buffer and the point\n",
    "                return\n",
    "            if isinstance(pnt, MultiPoint):\n",
    "                for single_pnt in pnt.geoms:\n",
    "                    self.inter_pnt_dic['geometry'].append(single_pnt)\n",
    "                    self.inter_pnt_dic['name'].append(row.name)\n",
    "                return\n",
    "            # If it is first or end continue OR if there is no intersection between the two lines\n",
    "            if len(pnt.coords) == 0 or pnt.coords[0] == line_1.geometry.coords[0] or pnt.coords[0] == \\\n",
    "                    line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            self.inter_pnt_dic['geometry'].append(pnt)\n",
    "            self.inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "\n",
    "    def update_names(self, org_gpd: GeoDataFrame):\n",
    "        \"\"\"\n",
    "        It updates the name of those lost their name during the previous process\n",
    "        :param org_gpd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df1 = self.my_network\n",
    "        # Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "        df3 = df1[df1['name'].notna()]\n",
    "        df4 = df1[df1['name'].isna()]\n",
    "\n",
    "        # use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "        old_index = 'old_index'\n",
    "        df4_as_buffer = GeoDataFrame(geometry=df4['geometry'].buffer(distance=2, cap_style=2), crs=project_crs)\n",
    "        df = gpd.sjoin(df4_as_buffer,\n",
    "                        org_gpd)  # for spatial join use buffer around each polyline.that provide better result\n",
    "        df.index.name = old_index\n",
    "        df['geometry'] = df4['geometry']  # bring the dataframe into linestring format\n",
    "        df.reset_index(inplace=True)  # To be consistent with the following code and other dataframe\n",
    "        # Create a new dictionary to store the updated data.\n",
    "        dic_str_data = []\n",
    "\n",
    "        def return_street_name(aplcnts_tst):\n",
    "            \"\"\"\n",
    "            1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "            2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "            3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "            4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "            :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            count_names = aplcnts_tst['name'].value_counts().sort_values(ascending=False)\n",
    "            if len(count_names) == 1:\n",
    "                # there is only one name\n",
    "                my_data = aplcnts_tst.iloc[0]\n",
    "            elif count_names[1] - count_names[0] > 1:\n",
    "                # The highest number of polylines with the same name are bigger at least in 2:\n",
    "                my_data = aplcnts_tst[aplcnts_tst['name'] == count_names.index[0]].iloc[0]\n",
    "            else:\n",
    "                # otherwise filter those with the most popular name or close to (-1)\n",
    "                str_to_wrk_on = aplcnts_tst[\n",
    "                    aplcnts_tst['name'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "                buffer_0 = GeoDataFrame(\n",
    "                    geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance=20, cap_style=2)],\n",
    "                    crs=project_crs)  # Buffer around the polyline without name\n",
    "\n",
    "                streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on[\n",
    "                                                                    'index_right'])].reset_index()  # Get all the applicants polylines and create buffer around\n",
    "                buffer_1 = GeoDataFrame(geometry=streets_right_geo.buffer(distance=20, cap_style=2))\n",
    "                streets_right_geo['area'] = gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "                groupy = streets_right_geo.groupby('name')\n",
    "                my_data_0 = \\\n",
    "                    groupy.get_group(groupy['area'].sum().sort_values(ascending=False).index[0]).sort_values(\n",
    "                        by='area',\n",
    "                        ascending=False).iloc[\n",
    "                        0]\n",
    "                # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "                my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "            # Populate the new dictionary with relevant data\n",
    "            dic_str_data.append(my_data.to_list())\n",
    "\n",
    "        _ = df.groupby(old_index).apply(return_street_name)\n",
    "        # convert the dictionary into a dataframe.\n",
    "        updated_df = GeoDataFrame(data=dic_str_data, columns=df.columns, crs=project_crs).drop(\n",
    "            columns='index_right').set_index(old_index)\n",
    "        updated_df['length'] = updated_df.length\n",
    "        self.my_network = GeoDataFrame(pd.concat([df3, updated_df]), crs=project_crs)\n",
    "\n",
    "\n",
    "# Roundabout\n",
    "class EnvEntity:\n",
    "    def __init__(self, network):\n",
    "        self.dead_end_fd = None\n",
    "        self.pnt_dead_end = None\n",
    "        self.pnt_dic = {}\n",
    "        self.first_last_dic = {'geometry': [], 'line_name': [], 'position': []}\n",
    "        self.network = network\n",
    "\n",
    "    def __populate_pnt_dic(self, point: type, name_of_line: str):\n",
    "        \"\"\"\n",
    "            Make \"pnt_dic\" contain a list of all the lines connected to each point.\n",
    "            :param point:\n",
    "            :param name_of_line:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        if not point in self.pnt_dic:\n",
    "            self.pnt_dic[point] = []\n",
    "        self.pnt_dic[point].append(name_of_line)\n",
    "\n",
    "    def __send_pnts(self, temp_line: GeoSeries):\n",
    "        \"\"\"\n",
    "            # Send the first and the last points to populate_pnt_dic\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        my_geom = temp_line['geometry']\n",
    "        self.__populate_pnt_dic(my_geom.coords[0], temp_line.name)\n",
    "        self.__populate_pnt_dic(my_geom.coords[-1], temp_line.name)\n",
    "\n",
    "    def get_deadend_gdf(self, delete_short: int = 30) -> GeoDataFrame:\n",
    "        self.network.apply(self.__send_pnts, axis=1)\n",
    "\n",
    "        deadend_list = [item[1][0] for item in self.pnt_dic.items() if len(item[1]) == 1]\n",
    "        pnt_dead_end_0 = [item for item in self.pnt_dic.items() if\n",
    "                            len(item[1]) == 1]  # Retain all the line points with deadened\n",
    "        self.pnt_dead_end = [Point(x[0]) for x in pnt_dead_end_0]\n",
    "        # Create shp file of deadened_pnts\n",
    "        geometry, line_name = 'geometry', 'line_name'\n",
    "        pnt_dead_end_df = GeoDataFrame(data=pnt_dead_end_0)\n",
    "        pnt_dead_end_df[geometry] = pnt_dead_end_df[0].apply(lambda x: Point(x))\n",
    "        pnt_dead_end_df[line_name] = pnt_dead_end_df[1].apply(lambda x: x[0])\n",
    "        pnt_dead_end_df.crs = project_crs\n",
    "        self.dead_end_fd = pnt_dead_end_df\n",
    "\n",
    "        if delete_short > 0:\n",
    "            # If it is necessary to eliminate dead-end short segments, it is  important to delete them from the network geodataframe.\n",
    "\n",
    "            deadend_gdf = self.network.loc[deadend_list]\n",
    "            self.network.drop(index=deadend_gdf[deadend_gdf.length < delete_short].index, inplace=True)\n",
    "            return deadend_gdf[deadend_gdf.length > delete_short]\n",
    "        return self.network.loc[deadend_list]\n",
    "\n",
    "    def update_the_current_network(self, temp_network):\n",
    "        r\"\"\"\n",
    "            Update the current network in the new changes\n",
    "            :param temp_network:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        new_network_temp = self.network.drop(index=temp_network.index)\n",
    "        self.network = GeoDataFrame(pd.concat([new_network_temp, temp_network]), crs=project_crs)\n",
    "        self.network['length'] = self.network.length\n",
    "        self.network = self.network[self.network['length'] > 1]\n",
    "\n",
    "\n",
    "class Roundabout(EnvEntity):\n",
    "    def __init__(self, network: GeoDataFrame):\n",
    "        EnvEntity.__init__(self, network)\n",
    "        self.pnt_dic = {}\n",
    "        self.centroid = self.__from_roundabout_to_centroid()\n",
    "        self.network.rename(columns={'name': 'str_name'}, inplace=True)\n",
    "\n",
    "    def __from_roundabout_to_centroid(self):\n",
    "        # Find the center of each roundabout\n",
    "        # create polygon around each polygon and union\n",
    "        round_about_buffer = round_about.to_crs(project_crs)['geometry'].buffer(cap_style=1, distance=10,\n",
    "                                                                                join_style=1).unary_union\n",
    "        dic_data = {'name': [], 'geometry': []}\n",
    "        if round_about_buffer.type == 'Polygon':  # In case we have only one polygon\n",
    "            dic_data['name'].append(0)\n",
    "            dic_data['geometry'].append(round_about_buffer.centroid)\n",
    "        else:\n",
    "            for ii, xx in enumerate(round_about_buffer.geoms):\n",
    "                dic_data['name'].append(ii)\n",
    "                dic_data['geometry'].append(xx.centroid)\n",
    "        centroid = GeoDataFrame(dic_data, crs=project_crs)\n",
    "        return centroid\n",
    "\n",
    "    def __first_last_pnt_of_line(self, row: GeoSeries):\n",
    "        r\"\"\"\n",
    "        It get geometry of line and fill the first_last_dic with the first and last point and the name of the line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        geo = list(row['geometry'].coords)\n",
    "        self.first_last_dic['geometry'].extend([Point(geo[0]), Point(geo[-1])])\n",
    "        self.first_last_dic['line_name'].extend([row.name] * 2)\n",
    "        self.first_last_dic['position'].extend([0, -1])\n",
    "\n",
    "    def deadend(self):\n",
    "        r\"\"\"\n",
    "        remove not connected line shorter than 100 meters and then return deadend_list lines and their endpoints (as another file)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Find the first and last points\n",
    "\n",
    "        # Get deadend_gdf\n",
    "        deadend_gdf = self.get_deadend_gdf()\n",
    "\n",
    "        # Create gdf of line points with the reference to the line they belong\n",
    "        deadend_gdf.apply(self.__first_last_pnt_of_line, axis=1)\n",
    "        first_last_gdf = GeoDataFrame(self.first_last_dic, crs=project_crs)\n",
    "\n",
    "        return deadend_gdf, first_last_gdf\n",
    "\n",
    "    def __update_geometry(self, cur, s_join):\n",
    "        r\"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cur['highway'] == 'footway':\n",
    "            # Don't snap footway to roundabout\n",
    "            return cur['geometry']\n",
    "        # Get only the points that are deadened\n",
    "        points_lines = [item for item in s_join[s_join['line_name'] == cur.name].iterrows() if\n",
    "                        item[1]['geometry'] in self.pnt_dead_end]\n",
    "        if len(points_lines) == 0:\n",
    "            # No roundabout nearby\n",
    "            return cur['geometry']\n",
    "        # get the line geometry to change the first and/ or last point\n",
    "        geo_cur = list(cur['geometry'].coords)\n",
    "\n",
    "        # iterate over the deadened points  near roundabout\n",
    "        for ind in range(len(points_lines)):\n",
    "            points_line = points_lines[ind]\n",
    "            geo_cur[points_line[1]['position']] = \\\n",
    "                self.centroid.loc[points_line[1]['index_right']]['geometry'].coords[\n",
    "                    0]\n",
    "        return LineString(geo_cur)\n",
    "\n",
    "    def my_spatial_join(self, deadend_lines, deadend_pnts, line_name):\n",
    "        # Spatial join between roundabout centroid to nearby dead end lines\n",
    "\n",
    "        s_join = gpd.sjoin_nearest(left_df=deadend_pnts, right_df=self.centroid, how='left', max_distance=100,\n",
    "                                    distance_col='dist').dropna(subset='dist')\n",
    "\n",
    "        # Deadened lines from both lines should be removed\n",
    "        lines_to_delete_test = s_join['line_name'].unique()  # all the Deadened lines close to roundabout\n",
    "\n",
    "        # All deadened lines from both lines\n",
    "        deads_both_side = self.dead_end_fd['line_name'].value_counts()\n",
    "        deads_both_side = deads_both_side[deads_both_side == 2]\n",
    "\n",
    "        # Remove this lines from the database\n",
    "        lines_to_delete = deads_both_side[deads_both_side.index.isin(lines_to_delete_test)]\n",
    "\n",
    "        self.network = self.network[\n",
    "            ~((self.network[line_name].isin(lines_to_delete.index)) & (self.network.length < 300))]\n",
    "        deadend_lines = deadend_lines[\n",
    "            ~((deadend_lines[line_name].isin(lines_to_delete.index)) & (deadend_lines.length < 300))]\n",
    "        # Update the geometry so the roundabout will be part of the line geometry\n",
    "        change_geo = deadend_lines.copy()\n",
    "\n",
    "        change_geo['geometry'] = change_geo.apply(lambda x: self.__update_geometry(x, s_join), axis=1)\n",
    "\n",
    "        return change_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update topology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1627/1627 [00:00<00:00, 8584.20it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 135.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update roundabout\n",
      "Update topology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1642/1642 [00:00<00:00, 8656.72it/s]\n",
      "100%|██████████| 93/93 [00:00<00:00, 140.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate intersections\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "new_gpd = gpd.read_file(f'{data_folder}/{output_network_name}_simp')\n",
    "# add unique name tags to any unnamed streets\n",
    "count = 1\n",
    "for r in range(len(new_gpd['name'])):\n",
    "    current_name = new_gpd.iloc[r]['name']\n",
    "    if current_name == '' or current_name == None:\n",
    "        new_gpd.iloc[r, new_gpd.columns.get_loc('name')] = f'unnamed_{count}'\n",
    "        count += 1\n",
    "\n",
    "obj_intersection = Intersection(new_gpd, num)\n",
    "obj_intersection.intersection_network()\n",
    "obj_intersection.update_names(new_gpd)\n",
    "line_name = 'line_name'\n",
    "if is_junction:\n",
    "    print('Update roundabout')\n",
    "    exist_data = obj_intersection.my_network.reset_index().reset_index(names=line_name)\n",
    "    my_roundabout = Roundabout(exist_data)\n",
    "    deadend_lines, deadend_pnts = my_roundabout.deadend()\n",
    "\n",
    "    # update the current network\n",
    "    change_geo = my_roundabout.my_spatial_join(deadend_lines, deadend_pnts, line_name)\n",
    "    my_roundabout.update_the_current_network(change_geo)\n",
    "\n",
    "    my_roundabout.network.drop_duplicates(subset=line_name, inplace=True)\n",
    "    # Improve roundabout\n",
    "    # First buffer around centroid\n",
    "    centr_name = 'centr_name'\n",
    "    buffer_around_centroid = my_roundabout.centroid['geometry'].buffer(cap_style=1, distance=30)\n",
    "\n",
    "    # s_join between buffer to lines (reset index to retain the original centroid name which can apper more than one in the results). always stay with data you need and with understandable name\n",
    "    roundabout_with_lines = \\\n",
    "        gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_centroid, crs=project_crs).reset_index(),\n",
    "                    right_df=my_roundabout.network[['geometry', line_name]]).drop_duplicates(\n",
    "            subset=['index', line_name]).rename(columns={\"index\": centr_name})[['geometry', line_name, centr_name]]\n",
    "\n",
    "    # To facilitate the searching process\n",
    "    my_roundabout.network.set_index(line_name, inplace=True)\n",
    "    # To facilitate easy access to point centroid geometry data, it is advisable to store the information in an object that provides efficient retrieval.\n",
    "    pnt_centroid_temp = my_roundabout.centroid['geometry']\n",
    "    #  Group the data by centroid\n",
    "    for center_line in roundabout_with_lines.groupby(centr_name):\n",
    "        #  Iterate over each group after performing a groupby() operation\n",
    "        for center in center_line[1].itertuples():\n",
    "            # Find the line that connects to the current centroid and obtain its vertices\n",
    "            line_to_test = my_roundabout.network.loc[center[2]]\n",
    "            vertices_line = list(line_to_test['geometry'].coords)\n",
    "            pnt_test = [vertices_line[0], vertices_line[-1]]\n",
    "            # To determine if the current line is already connected to the current centroid,.\n",
    "            is_connected = my_roundabout.centroid[\n",
    "                my_roundabout.centroid['geometry'].isin([Point(pnt_test[0]), Point(pnt_test[-1])])]\n",
    "            if len(is_connected) > 0 and center[3] in is_connected['name']:\n",
    "                continue\n",
    "\n",
    "            if len(vertices_line) == 2:\n",
    "                vertices_line.insert(1, pnt_centroid_temp[center[3]])\n",
    "            else:\n",
    "                my_list = [pnt_centroid_temp[center[3]].distance(Point(temp)) for temp in vertices_line]\n",
    "                # Find the minimum index\n",
    "                min_index = min(range(len(my_list)), key=my_list.__getitem__)\n",
    "                if min_index == 0:\n",
    "                    vertices_line.insert(0, pnt_centroid_temp[center[3]])\n",
    "                elif min_index == len(my_list) - 1:\n",
    "                    vertices_line.append(pnt_centroid_temp[center[3]])\n",
    "                else:\n",
    "                    vertices_line[min_index] = pnt_centroid_temp[center[3]]\n",
    "            new_geo = LineString(vertices_line)\n",
    "            my_roundabout.network.at[center[2], 'geometry'] = new_geo\n",
    "\n",
    "    new_network1 = my_roundabout.network.reset_index()\n",
    "    new_network1.drop(columns='index', inplace=True)\n",
    "\n",
    "\n",
    "    # Function to remove self-intersecting LineString geometries\n",
    "    def remove_self_intersecting(line):\n",
    "        return line.is_simple\n",
    "\n",
    "\n",
    "    # Apply the function to filter out self-intersecting geometries\n",
    "    new_network2 = new_network1[new_network1['geometry'].apply(remove_self_intersecting)]\n",
    "else:\n",
    "    new_network2 = obj_intersection.my_network.reset_index()\n",
    "\n",
    "extend_lines_f = extend_lines(new_network2, 100)\n",
    "extend_lines_f['length'] = extend_lines_f.length\n",
    "\n",
    "obj_intersection_1 = Intersection(extend_lines_f.copy(), 1)\n",
    "obj_intersection_1.intersection_network()\n",
    "obj_intersection_1.my_network.rename(columns={'str_name': 'name'},\n",
    "                                        inplace=True)  # 'str_name' become 'str to be compatible with other previous networks\n",
    "new_gpd.rename(columns={'str_name': 'name'}, inplace=True)\n",
    "obj_intersection_1.update_names(org_gpd=new_gpd)\n",
    "\n",
    "# Clear short segments\n",
    "final2 = EnvEntity(obj_intersection_1.my_network.reset_index())\n",
    "final2.update_the_current_network(final2.get_deadend_gdf(delete_short=100))\n",
    "\n",
    "# Aggregation\n",
    "print('Aggregate intersections')\n",
    "network = final2.network\n",
    "\n",
    "# 1. Get the first/start of each line\n",
    "# Extract unique start and end points from all LineStrings\n",
    "geometry = 'geometry'\n",
    "index_right = 'index_right'\n",
    "all_points = network[geometry].apply(lambda line: [Point(line.coords[0]), Point(line.coords[-1])]).explode()\n",
    "# # Create a GeoSeries of unique points\n",
    "unique_points = GeoDataFrame(geometry=gpd.GeoSeries(all_points).unique(), crs=project_crs)\n",
    "# save data\n",
    "\n",
    "# 2. Make sure I have the name of the lines associated with these lines\n",
    "pnts_line_name = unique_points.sjoin(network)[[index_right, geometry]].reset_index().dissolve(by='index',\n",
    "                                                                                                aggfunc=lambda\n",
    "                                                                                                    x: x.tolist())\n",
    "pnts_line_name['num_of_lines'] = pnts_line_name[index_right].apply(len)  # count the number of lines for each point\n",
    "\n",
    "# 3. Use DBSCAN with 20 meters threshold\n",
    "# Extract coordinates for DBSCAN\n",
    "coordinates = pnts_line_name.geometry.apply(lambda point: (point.x, point.y)).tolist()\n",
    "dbscan = DBSCAN(eps=40, min_samples=2)\n",
    "pnts_line_name['group'] = dbscan.fit_predict(coordinates)\n",
    "lines_to_update = pnts_line_name[pnts_line_name['group'] > -1]\n",
    "\n",
    "\n",
    "# if you want to save the files\n",
    "def save_points_file(data, path):\n",
    "    \"\"\"\n",
    "    The function get a data, arrange columns, convert list to string and export  it into a shpfile\n",
    "    :param data:\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    col_of_lists_as_str = 'col_of_lists_as_str'\n",
    "    data[col_of_lists_as_str] = data[index_right].apply(str)\n",
    "    data.drop(columns=[index_right]).to_file(path)\n",
    "    data.drop(columns=[col_of_lists_as_str], inplace=True)\n",
    "\n",
    "\n",
    "# 4.1.Find the point with the max number of connected lines, if it is one use it otherwise uses the average\n",
    "# Find the maximum 'num' value for each group\n",
    "num = 'num_of_lines'\n",
    "group_name = 'group'\n",
    "new_geometry = 'new_geometry'\n",
    "max_values_per_group = lines_to_update.groupby('group')['num_of_lines'].max()\n",
    "# Filter rows with the maximum 'num' value for each group\n",
    "result_gdf = lines_to_update[\n",
    "    lines_to_update.set_index([group_name, num]).index.isin(list(max_values_per_group.items()))]\n",
    "\n",
    "\n",
    "# Custom aggregation function to calculate the average point for each group\n",
    "def calculate_average_point(group):\n",
    "    x_mean = group.x.mean()\n",
    "    y_mean = group.y.mean()\n",
    "    return Point(x_mean, y_mean)\n",
    "\n",
    "\n",
    "# Apply the custom aggregation function to calculate average points per group\n",
    "lines_to_update2 = lines_to_update.set_index(group_name)\n",
    "lines_to_update2['new_geometry'] = result_gdf.groupby(group_name)[geometry].apply(calculate_average_point)\n",
    "\n",
    "# 4.2 Among whom are updated remove every line the start and last point are the same\n",
    "# Get all the lines going to be deleted\n",
    "lines_to_delete = []\n",
    "\n",
    "def update_lines_to_delete(row):\n",
    "    # explode the lines names within each row list to separate rows\n",
    "    lines_to_update_tmep = row[index_right].explode()\n",
    "\n",
    "    # Identify rows with duplicate values\n",
    "    lines_to_delete.extend(lines_to_update_tmep[lines_to_update_tmep.duplicated()].tolist())\n",
    "\n",
    "lines_to_update2.groupby(level=group_name).apply(update_lines_to_delete)\n",
    "\n",
    "# remove lines their geometry not going to change\n",
    "lines_to_update3 = lines_to_update2[lines_to_update2[geometry] != lines_to_update2[new_geometry]]\n",
    "\n",
    "# 4.3 Change the point of each line with new point\n",
    "network_new = network[~network.index.isin(lines_to_delete)]\n",
    "\n",
    "def update_network_with_aggregated_point(group):\n",
    "    lines_in_group = group.explode(index_right)\n",
    "\n",
    "    def update_one_line(points_data):\n",
    "        if points_data.name not in lines_to_delete:\n",
    "            updated_line_geo = network_new.loc[points_data.name]\n",
    "            line_coords = updated_line_geo.geometry.coords\n",
    "            if Point(line_coords[0]) == points_data.geometry:\n",
    "                network_new.at[points_data.name, geometry] = LineString(\n",
    "                    [points_data[new_geometry]] + line_coords[1:])\n",
    "            elif Point(line_coords[-1]) == points_data.geometry:\n",
    "                network_new.at[points_data.name, geometry] = LineString(\n",
    "                    line_coords[:-1] + [points_data[new_geometry]])\n",
    "            else:\n",
    "                print(points_data)\n",
    "                print(lines_in_group)\n",
    "\n",
    "    lines_in_group.set_index(index_right).apply(update_one_line, axis=1)\n",
    "\n",
    "\n",
    "lines_to_update3.groupby(level=group_name).apply(update_network_with_aggregated_point)\n",
    "network_new.drop(columns=[line_name, 'bearing']).to_file(f'{data_folder}/{output_network_name}_network_new')\n",
    "final2.network.drop(columns=[line_name, 'bearing']).to_file(f'{data_folder}/{output_network_name}_network')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiahua-connectivity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
